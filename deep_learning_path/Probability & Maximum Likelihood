{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPozD9vb4f3WZkXBB7RLYDi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Probability & Likelihood Logic\n","\n","In the process of training machine learning models, our primary goal is to identify the parameters that enable the most accurate mapping from given inputs to their corresponding outputs, tailored to the specific task at hand. This objective necessitates the utilization of a training dataset, denoted as $(x_i, y_i)$, comprising pairs of inputs and their corresponding desired outputs.\n","\n","To quantify the discrepancy between the predictions made by the model, represented as $f[x_i,\\phi]$, and the actual outputs $y_i$, we employ a function known as the loss or cost function, symbolized as $L[\\phi]$. This function produces a singular numerical value that encapsulates the degree of mismatch between predicted values and the ground-truth outputs.\n","\n","The training process involves an optimization task where we seek the values of the parameters $\\phi$ that minimize the loss function, thereby ensuring the closest possible alignment between the model-generated predictions and the true outputs.\n","\n","## Maximum Likelihood Estimation\n","\n","In the context of model training, consider a model $f[x, \\phi]$ characterized by parameters $\\phi$ that generates an output given an input $x$. Initially, it might be assumed that the model directly predicts an output $y$. However, we now adopt a perspective where the model is interpreted as computing a conditional probability distribution $Pr(y|x)$, indicating the probability of possible outputs $y$ given an input $x$. This approach, rooted in Bayesian statistics, leverages conditional probability to refine probability estimates in light of new evidence.\n","\n","Conditional probability, a pivotal concept in the realms of probability theory and statistics, evaluates the likelihood of an event's occurrence contingent upon the occurrence of another event. If we consider two events, $A$ and $B$, within the same probability space, the conditional probability of $A$ occurring given that $B$ has already occurred is denoted as $P(A|B)$ and is defined mathematically as follows:\n","\n","$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n","\n","where:\n"," - $P(A|B)$ represents the conditional probability of event $A$ given event $B$,\n"," - $P(A \\cap B)$ denotes the probability of both events $A$ and $B$ occurring simultaneously,\n"," - $P(B)$ signifies the probability of event $B$ happening\n","\n","During the training phase, the model is encouraged to ensure that each output $y_i$ is assigned a high probability by the distribution $Pr(y_i|x_i)$, calculated based on the corresponding input $x_i$.\n","\n","For establishing a probability distribution:\n","\n","- Initially, a parametric distribution $Pr(y|\\theta)$, defined over the output domain $y$, is selected.\n","- The model, through its computations, determines one or more parameters $\\theta$ of this distribution.\n","\n","The aim is to adjust the model parameters $\\phi$ to maximize the collective probability across all training instances $I$, formalized as:\n","\n","$$\\hat{\\phi} = \\underset{\\phi}{\\mathrm{argmax}} \\left[ \\prod_{i=1}^{I} \\Pr(y_i|x_i) \\right]$$\n","\n","$$= \\underset{\\phi}{\\mathrm{argmax}} \\left[ \\prod_{i=1}^{I} \\Pr(y_i|\\theta_i) \\right]$$\n","\n","$$= \\underset{\\phi}{\\mathrm{argmax}} \\left[ \\prod_{i=1}^{I} \\Pr(y_i|f(x_i, \\phi)) \\right]$$\n","\n","The term representing the combined probability is termed the likelihood of the parameters, leading to the designation of $\\hat{\\phi}$ as the criterion of maximum likelihood.\n","\n","## Maximizing Log-Likelihood\n","\n","Directly maximizing the likelihood criterion poses practical challenges, as the product of numerous small terms $Pr(y_i|f[x_i, \\phi])$ can result in extremely small values, complicating their representation in finite precision arithmetic. A solution to this predicament involves maximizing the log of the likelihood instead:\n","\n","$$\\underset{\\phi}{\\mathrm{argmax}} \\left[ \\log \\prod_{i=1}^{I} \\Pr(y_i|f(x_i, \\phi)) \\right]$$\n","\n","$$= \\underset{\\phi}{\\mathrm{argmax}} \\left[ \\sum_{i=1}^{I} \\log \\Pr(y_i|f(x_i, \\phi)) \\right]$$\n","\n","Enhancing the model parameters $\\phi$ to improve the log-likelihood equivalently enhances the original likelihood criterion. It is also deduced that the optimum points of both criteria coincide, implying the identification of the most suitable model parameters $\\hat{\\phi}$ is consistent across both approaches. Nevertheless, the log-likelihood offers a computational advantage by employing a summation of terms rather than a product, thereby mitigating issues related to finite precision representation.\n","\n","Conventionally, model fitting endeavors are formulated as minimization problems. To align with this convention, the maximum log-likelihood criterion is transformed into a minimization task by incorporating a negative sign, yielding the negative log-likelihood criterion:\n","\n","$$\\hat{\\phi} = \\underset{\\phi}{\\mathrm{argmin}} \\left[ - \\sum_{i=1}^{I} \\log \\Pr(y_i|f(x_i, \\phi)) \\right]$$\n","\n","$$= \\underset{\\phi}{\\mathrm{argmin}[L|\\phi|]}$$\n","\n","## Inference\n","\n","Subsequent to training, the model no longer directly forecasts outputs $y$ but rather delineates a probability distribution over potential outputs $y$. For inference purposes, where a specific estimate rather than a distribution is often desirable, the model outputs the distribution's mode:\n","\n","$$\\hat{y} = \\underset{y}{\\mathrm{argmax}[Pr(y|f(x,\\hat{\\phi})|)]}$$\n","\n","## Procedure for Loss Function Construction\n","\n","The methodology for devising loss functions for training data $(x_i,y_i)$, leveraging the maximum likelihood framework, encompasses the following steps:\n","\n","- Selection of an appropriate probability distribution $Pr(y|\\theta)$, covering the prediction domain $y$ with parameters $\\theta$.\n","\n","- Configuration of the machine learning model $f[x,\\phi]$ to predict one or more of these parameters, thereby establishing $\\theta = f[x,\\phi]$ and $Pr(y|\\theta) = Pr(y|f[x,\\phi])$.\n","\n","- Optimization of the model parameters $\\hat{\\phi}$ through minimization of the negative log-likelihood loss function across the training dataset pairs $(x_i,y_i)$:\n","\n","$$\\hat{\\phi} = \\underset{\\phi}{\\mathrm{argmin}} \\left[ - \\sum_{i=1}^{I} \\log \\Pr(y_i|f(x_i, \\phi)) \\right]$$\n","\n","$$= \\underset{\\phi}{\\mathrm{argmin}[L|\\phi|]}$$\n","\n","- For new test examples $x$, the model can either return the complete distribution $Pr(y|f[x,\\hat{\\phi}])$ or its maximum, depending on the specific inference requirements.\n","\n","\n","\n","\n"],"metadata":{"id":"ryqhuTSI4Kk7"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"9gM-M5842Qm3","executionInfo":{"status":"ok","timestamp":1710157936209,"user_tz":-180,"elapsed":1810,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"outputs":[],"source":["import numpy as np\n","from sklearn.linear_model import LinearRegression"]},{"cell_type":"code","source":["sample = np.array([[1],[2],[3],[4]])\n","print(f\"Input shape: {sample.shape}\")\n","groundTruth = np.array([2,4,6,8])\n","print(f\"Output shape: {groundTruth.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6j_SVx6HWxXD","executionInfo":{"status":"ok","timestamp":1710158040208,"user_tz":-180,"elapsed":16,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"a4f4e85a-8b3a-4445-b928-451a09d629b6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: (4, 1)\n","Output shape: (4,)\n"]}]},{"cell_type":"code","source":["model = LinearRegression()\n","model.fit(sample,groundTruth)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"8orvu3PaXMkH","executionInfo":{"status":"ok","timestamp":1710158101251,"user_tz":-180,"elapsed":14,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"00fb6284-85eb-4aab-fb43-b6d7e7a5db5d"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LinearRegression()"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["slope = model.coef_[0]\n","intercept = model.intercept_\n","print(f\"Slope --> {slope} with intercept --> {intercept}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OKlhqD7Xbez","executionInfo":{"status":"ok","timestamp":1710158139692,"user_tz":-180,"elapsed":257,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"06adb761-977e-44d2-8c00-9a552a0cb6f4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Slope --> 2.0 with intercept --> 0.0\n"]}]},{"cell_type":"markdown","source":["- Maximum Likelihood Estimation (MLE) example:"],"metadata":{"id":"Nfkux2P1Yhkr"}},{"cell_type":"code","source":["def Likelihood(groundTruth:np.ndarray,predictions:np.ndarray,sigma:int=1)->np.ndarray:\n","  \"\"\"\n","    Calculate the likelihood of the data given the model parameters.\n","    Assuming groundTruth follows a normal distribution around predictions with standard deviation sigma.\n","  \"\"\"\n","  constant = 1 / np.sqrt(2*np.pi*sigma**2)\n","  return np.prod(constant*np.exp(-((groundTruth-predictions)**2)/(2*sigma**2)))"],"metadata":{"id":"4ZOY-BxlXkwX","executionInfo":{"status":"ok","timestamp":1710158681880,"user_tz":-180,"elapsed":11,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Predicted values using the model parameters from before\n","predictions = model.predict(sample)"],"metadata":{"id":"M0ajQ3arZpRY","executionInfo":{"status":"ok","timestamp":1710158711540,"user_tz":-180,"elapsed":289,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["likelihoodValue = Likelihood(groundTruth,predictions)\n","print(f\"Likelihood: {likelihoodValue}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tf9NG3kNZwbb","executionInfo":{"status":"ok","timestamp":1710158748114,"user_tz":-180,"elapsed":12,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"b4a66526-d9cd-4a34-8b81-eef5f51c3b0f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Likelihood: 0.02533029591058445\n"]}]},{"cell_type":"markdown","source":["**Given the likelihood value 0.02533029591058445, it means that, under the assumptions of your model (errors follow a normal distribution with mean 0 and standard deviation '$\\sigma$'), this is the combined probability of observing all your ground truth values given the predictions made by your model.**"],"metadata":{"id":"FukazXxjelVQ"}},{"cell_type":"markdown","source":["- Maximizing Log-Likelihood example:"],"metadata":{"id":"MH8DOS4raBBx"}},{"cell_type":"code","source":["def LogLikelihood(groundTruth:np.ndarray,predictions:np.ndarray,sigma:int=1)->np.ndarray:\n","  \"\"\"\n","    Calculate the log likelihood of the data given the model parameters.\n","  \"\"\"\n","  constant = -0.5*np.log(2*np.pi*sigma**2)\n","  return np.sum(constant-((groundTruth-predictions)**2)/(2*sigma**2))"],"metadata":{"id":"LfggQ0tqZ5cn","executionInfo":{"status":"ok","timestamp":1710158880761,"user_tz":-180,"elapsed":13,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["logLikelihoodValue = LogLikelihood(groundTruth,predictions)\n","print(f\"Log-Likelihood: {logLikelihoodValue}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uG83Js6YePZp","executionInfo":{"status":"ok","timestamp":1710159920747,"user_tz":-180,"elapsed":8,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"6f9bd918-d94a-443f-8acb-4fd829934517"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Log-Likelihood: -3.6757541328186907\n"]}]},{"cell_type":"code","source":["testValue = np.array([[5]])\n","predictionValue = model.predict(testValue)\n","print(f\"Prediction: {predictionValue[0]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GMzHyt5naX8i","executionInfo":{"status":"ok","timestamp":1710159948627,"user_tz":-180,"elapsed":5,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"a17291dc-2656-4fbc-fc25-04003dc57045"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction: 10.0\n"]}]},{"cell_type":"markdown","source":["**real case example**:\n","\n","- We have observed heights of 5 individuals: [170,172,168,171,169] cm\n","- Our simple model (perhaps based on some basic assumptions or prior knowledge) predicts everyone's height to be around 170 cm\n","- We assume a standard deviation of 2 cm in our predictions, reflecting our uncertainty in the model\n","\n","**We will calculate the likelihood of observing the actual heights given these predictions**\n","\n","\n"],"metadata":{"id":"wDlg8RvyfLoM"}},{"cell_type":"code","source":["def Likelihood(groundTruth:np.ndarray,predictions:np.ndarray,sigma:int=1)->np.ndarray:\n","  \"\"\"\n","    Calculate the likelihood of the data given the model parameters.\n","    Assuming groundTruth follows a normal distribution around predictions with standard deviation sigma.\n","  \"\"\"\n","  constant = 1/np.sqrt(2*np.pi*sigma**2)\n","  exp = np.exp(-((groundTruth-predictions)**2)/(2*sigma**2))\n","  return np.prod(constant*exp)"],"metadata":{"id":"6xeyPhoGc90e","executionInfo":{"status":"ok","timestamp":1710161889145,"user_tz":-180,"elapsed":737,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["groundTruth = np.array([170,172,168,171,169]) # Observed heights\n","predictions = np.array([170,170,170,170,170]) # Model predictions (170 cm for each individual)\n","sigma = 2 # Standard deviation"],"metadata":{"id":"JUwoMI-jl4Gu","executionInfo":{"status":"ok","timestamp":1710161936914,"user_tz":-180,"elapsed":21,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["**The output will be a probability value indicating how likely these observed heights are given the model predictions. Remember, a higher likelihood value would suggest a better fit of the model to the observed data, within the context of the assumed error distribution.**"],"metadata":{"id":"-sGVdWbTmSND"}},{"cell_type":"code","source":["likelihoodValue = Likelihood(groundTruth,predictions,sigma)\n","print(f\"Likelihood Value: {likelihoodValue}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abDWd9GumDs9","executionInfo":{"status":"ok","timestamp":1710161964824,"user_tz":-180,"elapsed":8,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"73c82e4f-b194-438b-d1d0-13804e8cbd79"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Likelihood Value: 9.04757617747844e-05\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bh_SdmhAmKuH"},"execution_count":null,"outputs":[]}]}