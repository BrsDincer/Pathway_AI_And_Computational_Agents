{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM6iL/6uINmhA6hTjSH72gV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Loss Functions\n","\n","Loss functions, also known as cost functions, measure the inconsistency between the predicted output and the actual output in machine learning.\n","\n","## Mean Squared Error (MSE) Loss\n","\n","**Description**: It measures the average of the squares of the errors, i.e., the average squared difference between the estimated values and the actual value.\n","\n","$$\\text{MSE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n","\n","\n","where $y$ is the actual value, $\\hat{y}$ is the predicted value, and $n$ is the number of samples.\n","\n","## Mean Absolute Error (MAE) Loss\n","\n","**Description**: It measures the average of the absolute errors between the predicted values and the actual values. It is less sensitive to outliers compared to MSE.\n","\n","$$\\text{MAE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n","\n","## Cross-Entropy Loss (or Log Loss)\n","\n","**Description**: Commonly used in classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n","\n","$$\\text{Cross-Entropy}(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$\n","\n","$$\\text{Cross-Entropy}(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{M} y_{ic} \\log(\\hat{y}_{ic})$$\n","\n","where $M$ is the number of classes, $y_{ic}$ is a binary indicator of whether class label $c$ is the correct classification for observation $i$, and $\\hat{y}_{ic}$ is the predicted probability that observation $i$ is of class $c$.\n","\n","## Huber Loss\n","\n","**Description**: Often used in regression problems. It's less sensitive to outliers than the MSE as it combines MSE and MAE, using a parameter to switch between the two.\n","\n","$$\\text{Huber Loss}(y, \\hat{y}) = \\begin{cases}\n","\\frac{1}{2} (y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\le \\delta, \\\\\n","\\delta |y - \\hat{y}| - \\frac{1}{2} \\delta^2 & \\text{otherwise.}\n","\\end{cases}\n","$$\n","\n","where $\\delta$ is a hyperparameter to choose.\n","\n","## Hinge Loss\n","\n","**Description**: Commonly used for binary classification problems, such as with Support Vector Machines.\n","\n","$$\\text{Hinge Loss}(y, \\hat{y}) = \\max(0, 1 - y_i \\cdot \\hat{y}_i)$$\n","\n","where $y \\in {-1, 1}$ is the actual label and $\\hat{y}$ is the predicted label.\n","\n","## Categorical Crossentropy\n","\n","**Description**: A variant of Cross-Entropy Loss used when the target is categorical.\n","\n"],"metadata":{"id":"3S578XMe--jc"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"SWjgaW75-Sow","executionInfo":{"status":"ok","timestamp":1710508094756,"user_tz":-180,"elapsed":7366,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf"]},{"cell_type":"markdown","source":["## Mean Squared Error (MSE) Loss"],"metadata":{"id":"0IIiA71iSx1S"}},{"cell_type":"code","source":["# Mean Squared Error (MSE) Loss\n","def MSELoss(groundTruth:np.ndarray,predictions:np.ndarray)->np.ndarray:\n","  return np.mean(np.square(groundTruth-predictions))"],"metadata":{"id":"28m9cMYwOe2x","executionInfo":{"status":"ok","timestamp":1710508130706,"user_tz":-180,"elapsed":24,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["groundTruth = np.array([1,2,3,4,5])\n","predictions = np.array([1.5,2.5,3,3.5,4.5])\n","print(f\"Mean Squared Error: {MSELoss(groundTruth,predictions)}\")"],"metadata":{"id":"gDUQf9L2Sy9O","executionInfo":{"status":"ok","timestamp":1710509215075,"user_tz":-180,"elapsed":8,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"84b4a3d7-03ed-4217-e76e-5fd912d69c6e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error: 0.2\n"]}]},{"cell_type":"markdown","source":["## Mean Absolute Error (MAE) Loss"],"metadata":{"id":"h5TQrqS-SlqD"}},{"cell_type":"code","source":["# Mean Absolute Error (MAE) Loss\n","def MAELoss(groundTruth:np.ndarray,predictions:np.ndarray)->np.ndarray:\n","  return np.mean(np.abs(groundTruth-predictions))"],"metadata":{"id":"Yls5zqyNOr1I","executionInfo":{"status":"ok","timestamp":1710508173898,"user_tz":-180,"elapsed":414,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["groundTruth = np.array([1,2,3,4,5])\n","predictions = np.array([1.5,2.5,3,3.5,4.5])\n","print(f\"Mean Absolute Error: {MAELoss(groundTruth,predictions)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1z5p4zQ4SmkM","executionInfo":{"status":"ok","timestamp":1710509198161,"user_tz":-180,"elapsed":14,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"6b306ad4-c420-4453-c3de-8005a2cbe028"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Absolute Error: 0.4\n"]}]},{"cell_type":"markdown","source":["## Cross-Entropy Loss (Binary Classification)"],"metadata":{"id":"XsNKnGgZSUmi"}},{"cell_type":"code","source":["# Cross-Entropy Loss (Binary Classification)\n","def BinaryCrossEntropy(groundTruth:np.ndarray,predictions:np.ndarray)->np.ndarray:\n","  epsilon = 1e-15\n","  predictions = np.clip(predictions,epsilon,1-epsilon)\n","  return -np.mean(groundTruth*np.log(predictions)+(1-groundTruth)*np.log(1-predictions))"],"metadata":{"id":"tZvPDG7lO2Uz","executionInfo":{"status":"ok","timestamp":1710508264097,"user_tz":-180,"elapsed":347,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["groundTruth = np.array([0,1,1,0,1])\n","predictions = np.array([0.1,0.9,0.8,0.4,0.5])\n","print(f\"Binary Cross-Entropy Loss: {BinaryCrossEntropy(groundTruth,predictions)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3jIEMOESWHQ","executionInfo":{"status":"ok","timestamp":1710509146918,"user_tz":-180,"elapsed":14,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"1c634a83-5a09-4d8e-9f3b-a40661e9da62"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Binary Cross-Entropy Loss: 0.32756747739115966\n"]}]},{"cell_type":"markdown","source":["## Huber Loss"],"metadata":{"id":"-2yhZtAIQRgq"}},{"cell_type":"code","source":["# Huber Loss\n","def HuberLossWrapper(deltaValue:int|float):\n","  def HuberLoss(groundTruth:np.ndarray,predictions:np.ndarray)->np.ndarray:\n","    huber = tf.keras.losses.Huber(delta=deltaValue)\n","    return huber(groundTruth,predictions)\n","  return HuberLoss"],"metadata":{"id":"0dvwHAoLPMX8","executionInfo":{"status":"ok","timestamp":1710509065709,"user_tz":-180,"elapsed":9,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["groundTruth = tf.constant(\n","    [1,2,3,4,5],\n","    dtype=tf.float32\n",")\n","predictions = tf.constant(\n","    [1.5,2.5,3,3.5,4.5],\n","    dtype=tf.float32\n",")"],"metadata":{"id":"8Z2qb9YGROTE","executionInfo":{"status":"ok","timestamp":1710509065709,"user_tz":-180,"elapsed":7,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["huberLoss = HuberLossWrapper(deltaValue=1.0)\n","print(f\"Huber Loss:\\n{huberLoss(groundTruth,predictions).numpy()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M2HR8VJ2RZtM","executionInfo":{"status":"ok","timestamp":1710509067411,"user_tz":-180,"elapsed":6,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"4924e88f-0aec-4da4-919c-4879d2cdf132"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Huber Loss:\n","0.10000000149011612\n"]}]},{"cell_type":"markdown","source":["## Hinge Loss"],"metadata":{"id":"bfUJV1e4QBrx"}},{"cell_type":"code","source":["# Hinge Loss\n","def HingeLoss(groundTruth:np.ndarray,predictions:np.ndarray)->np.ndarray:\n","  return np.mean(np.maximum(0,1-groundTruth*predictions))"],"metadata":{"id":"BBBecTF6PiBk","executionInfo":{"status":"ok","timestamp":1710508441971,"user_tz":-180,"elapsed":476,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["groundTruth = np.array([-1,1,1,-1,1])\n","predictions = np.array([-0.8,0.8,0.3,-0.5,-0.1])"],"metadata":{"id":"OoQdhmxcP3u0","executionInfo":{"status":"ok","timestamp":1710508537281,"user_tz":-180,"elapsed":481,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["print(f\"Hinge Loss:\\n{HingeLoss(groundTruth,predictions)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H4QzZXWZP_w1","executionInfo":{"status":"ok","timestamp":1710508537898,"user_tz":-180,"elapsed":5,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"7a039c07-d56e-420f-a242-3a33489ccf20"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Hinge Loss:\n","0.54\n"]}]},{"cell_type":"markdown","source":["## Categorical Crossentropy"],"metadata":{"id":"hMyZR60TQYF_"}},{"cell_type":"code","source":["groundTruth = tf.constant(\n","    [\n","        [0,1,0],\n","        [1,0,0],\n","        [0,0,1]\n","    ],\n","    dtype=tf.float32\n",")\n","predictions = tf.constant(\n","    [\n","        [0.05,0.95,0],\n","        [0.9,0.05,0.05],\n","        [0.1,0.1,0.8]\n","    ],\n","    dtype=tf.float32\n",")\n","print(f\"Shape of Ground Truth: {groundTruth.shape}\")\n","print(f\"Shape of predictions: {predictions.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sG5-GlzqQKJm","executionInfo":{"status":"ok","timestamp":1710508732262,"user_tz":-180,"elapsed":9,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"f50a22d0-7901-41b1-8bf9-b6ee395b442b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of Ground Truth: (3, 3)\n","Shape of predictions: (3, 3)\n"]}]},{"cell_type":"code","source":["loss = tf.keras.losses.CategoricalCrossentropy()\n","print(f\"Categorical Cross-Entropy: {loss(groundTruth,predictions).numpy()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0-OJrlrQv9y","executionInfo":{"status":"ok","timestamp":1710508788700,"user_tz":-180,"elapsed":477,"user":{"displayName":"Baris Dincer","userId":"16400678175301643843"}},"outputId":"e5eabd15-4291-46ff-e0fc-b99a30777428"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Categorical Cross-Entropy: 0.12659913301467896\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"becu_E_dRMRE"},"execution_count":null,"outputs":[]}]}